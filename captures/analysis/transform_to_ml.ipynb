{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2889bb48-72c8-467d-8aa6-769ad31e5624",
   "metadata": {},
   "source": [
    "# Transform Traffic Captures to ML Format\n",
    "\n",
    "This notebook converts raw traffic capture files into pickle format compatible with the ExplainWF framework for Website Fingerprinting attacks.\n",
    "\n",
    "## Input Format\n",
    "- Raw traffic traces from `../../data/traffic_captures/` or `../../data/reduced_list/`\n",
    "- Each file contains traces in format: `<url> <num_packets> <timestamp1>:<size1> <timestamp2>:<size2> ...`\n",
    "- Negative sizes = outgoing packets (client-to-server)\n",
    "- Positive sizes = incoming packets (server-to-client)\n",
    "\n",
    "## Output Format\n",
    "- Pickle file compatible with ExplainWF: https://github.com/explainwf-popets2023/explainwf-popets2023.github.io/tree/main/data\n",
    "- Contains tuple: `(X_train, X_test, y_train, y_test)`\n",
    "- Each sample is a dictionary with `cells` key containing list of `[rel_time, direction, direction, volume]`\n",
    "- Direction: 0 = outgoing, 1 = incoming\n",
    "- Relative time in seconds from first packet\n",
    "\n",
    "## Processing Steps\n",
    "1. Read all trace files from input directory\n",
    "2. Parse each line (up to 200 instances per website)\n",
    "3. Convert to relative timestamps (seconds from start)\n",
    "4. Filter websites with at least 20 instances\n",
    "5. Split into 60% train / 40% test with stratification\n",
    "6. Save as pickle file\n",
    "\n",
    "## Usage\n",
    "Set the `scenario` variable below to the configuration you want to process, then run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88006e51-70bd-475d-a766-6d2b42b81f29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_pkl(tcp_path, pkl_name, time_added=0):\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = os.path.dirname(pkl_name)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    for path, subdirs, files in os.walk(tcp_path):\n",
    "        index = 1\n",
    "        for file in files:\n",
    "            \n",
    "            instance = 1\n",
    "            n = 200\n",
    "            DIRECTION = {\"client-to-server\": -1, \"server-to-client\": 1}\n",
    "            instance = 1\n",
    "            \n",
    "            with open(os.path.join(path, file), \"r\") as f:\n",
    "                print(\"({}/{}) Reading : {}\".format(index, len(files), file))\n",
    "                for line in f:\n",
    "                    points = line.split(\" \")[2:]\n",
    "                    base_time = points[0].split(\":\")[0]\n",
    "                    base_time = float(base_time) if \".\" in base_time else int(base_time)\n",
    "                    file_content = \"\"\n",
    "                    cells = []\n",
    "                    for point in points:\n",
    "    \n",
    "                        time = point.split(\":\")[0]\n",
    "                        time = float(time) if \".\" in time else int(time)\n",
    "                        \n",
    "                        rel_time = (time - base_time) / 1000.0\n",
    "    \n",
    "                        # in our format, -1 is OUT, 1 is IN\n",
    "                        direction = np.sign(float(point.split(\":\")[1]))\n",
    "    \n",
    "                        direction = 0 if direction == DIRECTION[\"client-to-server\"] else 1\n",
    "    \n",
    "    \n",
    "                        volume = int(np.abs(float(point.split(\":\")[1])))\n",
    "    \n",
    "                        cells.append([rel_time, direction,direction,  volume])\n",
    "                    \n",
    "                    filtered_cells = [c for c in cells if c[0] <= cells[-1][0] - time_added]\n",
    "                    labels.append(file)\n",
    "                    data.append({\"cells\": filtered_cells})\n",
    "                    \n",
    "                    instance += 1\n",
    "    \n",
    "                    if instance > n:\n",
    "                       break\n",
    "                index += 1\n",
    "                if instance <= n:\n",
    "                    print(f\"[ERROR] {file} has only {instance-1} instances\")\n",
    "\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "\n",
    "        # Filter data and labels\n",
    "    filtered_data = []\n",
    "    filtered_labels = []\n",
    "    \n",
    "    s = set(labels)\n",
    "    ds = dict(zip(s,range(len(s))))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for d, l in zip(data, labels):\n",
    "        if label_counts[l] >= 20:\n",
    "            filtered_data.append(d)\n",
    "            filtered_labels.append(ds[l])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        filtered_data,\n",
    "        filtered_labels,\n",
    "        train_size=0.6,\n",
    "        shuffle=True,\n",
    "        stratify=filtered_labels,\n",
    "    )\n",
    "\n",
    "    # Assuming args.output_file is a file path\n",
    "    with open(pkl_name, 'wb') as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test), f)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16f587",
   "metadata": {},
   "source": [
    "## Example: Process a Single Configuration\n",
    "\n",
    "Change the `scenario` variable to match the directory name in `../../data/reduced_list/` or `../../data/traffic_captures/`.\n",
    "\n",
    "Available scenarios include:\n",
    "- `configuration00_default` - Default Nym setup\n",
    "- `configuration01_lqp10` - Low queue parameter 10\n",
    "- `nym_defence_wtfpad` - Nym with WTF-PAD defense\n",
    "- `tor_defence_front` - Tor with FRONT defense\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae5f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/58) Reading : soundcloud.com\n",
      "(2/58) Reading : facebook.com\n",
      "(2/58) Reading : facebook.com\n",
      "(3/58) Reading : springer.com\n",
      "(3/58) Reading : springer.com\n",
      "(4/58) Reading : latimes.com\n",
      "(4/58) Reading : latimes.com\n",
      "(5/58) Reading : medium.com\n",
      "(5/58) Reading : medium.com\n",
      "(6/58) Reading : github.com\n",
      "(6/58) Reading : github.com\n",
      "(7/58) Reading : yahoo.com\n",
      "(7/58) Reading : yahoo.com\n",
      "(8/58) Reading : weibo.com\n",
      "(8/58) Reading : weibo.com\n",
      "(9/58) Reading : linkedin.com\n",
      "(9/58) Reading : linkedin.com\n",
      "(10/58) Reading : unsplash.com\n",
      "(10/58) Reading : unsplash.com\n",
      "(11/58) Reading : yelp.com\n",
      "(11/58) Reading : yelp.com\n",
      "(12/58) Reading : baidu.com\n",
      "(12/58) Reading : baidu.com\n",
      "(13/58) Reading : nature.com\n",
      "(13/58) Reading : nature.com\n",
      "(14/58) Reading : techcrunch.com\n",
      "(14/58) Reading : techcrunch.com\n",
      "(15/58) Reading : free.fr\n",
      "(15/58) Reading : free.fr\n",
      "(16/58) Reading : dailymail.co.uk\n",
      "(16/58) Reading : dailymail.co.uk\n",
      "(17/58) Reading : forbes.com\n",
      "(17/58) Reading : forbes.com\n",
      "(18/58) Reading : ebay.com\n",
      "(18/58) Reading : ebay.com\n",
      "(19/58) Reading : etsy.com\n",
      "(19/58) Reading : etsy.com\n",
      "(20/58) Reading : huffingtonpost.com\n",
      "(20/58) Reading : huffingtonpost.com\n",
      "(21/58) Reading : creativecommons.org\n",
      "(21/58) Reading : creativecommons.org\n",
      "(22/58) Reading : datatracker.ietf.org\n",
      "(22/58) Reading : datatracker.ietf.org\n",
      "(23/58) Reading : berkeley.edu\n",
      "(23/58) Reading : berkeley.edu\n",
      "(24/58) Reading : google.com\n",
      "(24/58) Reading : google.com\n",
      "(25/58) Reading : telegraph.co.uk\n",
      "(25/58) Reading : telegraph.co.uk\n",
      "(26/58) Reading : npr.org\n",
      "(26/58) Reading : npr.org\n",
      "(27/58) Reading : addtoany.com\n",
      "(27/58) Reading : addtoany.com\n",
      "(28/58) Reading : gov.uk\n",
      "(28/58) Reading : gov.uk\n",
      "(29/58) Reading : paypal.com\n",
      "(29/58) Reading : paypal.com\n",
      "(30/58) Reading : slideshare.net\n",
      "(30/58) Reading : slideshare.net\n",
      "(31/58) Reading : imgur.com\n",
      "(31/58) Reading : imgur.com\n",
      "(32/58) Reading : twitter.com\n",
      "(32/58) Reading : twitter.com\n",
      "(33/58) Reading : mozilla.org\n",
      "(33/58) Reading : mozilla.org\n",
      "(34/58) Reading : w3.org\n",
      "(34/58) Reading : w3.org\n",
      "(35/58) Reading : theguardian.com\n",
      "(35/58) Reading : theguardian.com\n",
      "(36/58) Reading : instagram.com\n",
      "(36/58) Reading : instagram.com\n",
      "(37/58) Reading : apple.com\n",
      "(37/58) Reading : apple.com\n",
      "(38/58) Reading : gnu.org\n",
      "(38/58) Reading : gnu.org\n",
      "(39/58) Reading : flickr.com\n",
      "(39/58) Reading : flickr.com\n",
      "(40/58) Reading : theverge.com\n",
      "(40/58) Reading : theverge.com\n",
      "(41/58) Reading : ted.com\n",
      "(41/58) Reading : ted.com\n",
      "(42/58) Reading : wordpress.org\n",
      "(42/58) Reading : wordpress.org\n",
      "(43/58) Reading : mit.edu\n",
      "(43/58) Reading : mit.edu\n",
      "(44/58) Reading : reddit.com\n",
      "(44/58) Reading : reddit.com\n",
      "(45/58) Reading : washingtonpost.com\n",
      "(45/58) Reading : washingtonpost.com\n",
      "(46/58) Reading : reuters.com\n",
      "(46/58) Reading : reuters.com\n",
      "(47/58) Reading : vimeo.com\n",
      "(47/58) Reading : vimeo.com\n",
      "(48/58) Reading : cnn.com\n",
      "(48/58) Reading : cnn.com\n",
      "(49/58) Reading : nih.gov\n",
      "(49/58) Reading : nih.gov\n",
      "(50/58) Reading : wikipedia.org\n",
      "(50/58) Reading : wikipedia.org\n",
      "(51/58) Reading : bbc.co.uk\n",
      "(51/58) Reading : bbc.co.uk\n",
      "(52/58) Reading : apache.org\n",
      "(52/58) Reading : apache.org\n",
      "(53/58) Reading : europa.eu\n",
      "(53/58) Reading : europa.eu\n",
      "(54/58) Reading : pinterest.com\n",
      "(54/58) Reading : pinterest.com\n",
      "(55/58) Reading : bing.com\n",
      "(55/58) Reading : bing.com\n",
      "(56/58) Reading : opera.com\n",
      "(56/58) Reading : opera.com\n",
      "(57/58) Reading : un.org\n",
      "(57/58) Reading : un.org\n",
      "(58/58) Reading : office.com\n",
      "(58/58) Reading : office.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'twitter.com': 0,\n",
       " 'opera.com': 1,\n",
       " 'washingtonpost.com': 2,\n",
       " 'imgur.com': 3,\n",
       " 'yelp.com': 4,\n",
       " 'linkedin.com': 5,\n",
       " 'unsplash.com': 6,\n",
       " 'creativecommons.org': 7,\n",
       " 'gov.uk': 8,\n",
       " 'berkeley.edu': 9,\n",
       " 'wordpress.org': 10,\n",
       " 'mozilla.org': 11,\n",
       " 'nih.gov': 12,\n",
       " 'free.fr': 13,\n",
       " 'wikipedia.org': 14,\n",
       " 'apple.com': 15,\n",
       " 'gnu.org': 16,\n",
       " 'flickr.com': 17,\n",
       " 'europa.eu': 18,\n",
       " 'etsy.com': 19,\n",
       " 'apache.org': 20,\n",
       " 'w3.org': 21,\n",
       " 'reddit.com': 22,\n",
       " 'forbes.com': 23,\n",
       " 'theguardian.com': 24,\n",
       " 'latimes.com': 25,\n",
       " 'office.com': 26,\n",
       " 'soundcloud.com': 27,\n",
       " 'yahoo.com': 28,\n",
       " 'huffingtonpost.com': 29,\n",
       " 'reuters.com': 30,\n",
       " 'techcrunch.com': 31,\n",
       " 'mit.edu': 32,\n",
       " 'slideshare.net': 33,\n",
       " 'facebook.com': 34,\n",
       " 'cnn.com': 35,\n",
       " 'instagram.com': 36,\n",
       " 'datatracker.ietf.org': 37,\n",
       " 'ted.com': 38,\n",
       " 'bing.com': 39,\n",
       " 'dailymail.co.uk': 40,\n",
       " 'nature.com': 41,\n",
       " 'addtoany.com': 42,\n",
       " 'springer.com': 43,\n",
       " 'weibo.com': 44,\n",
       " 'google.com': 45,\n",
       " 'vimeo.com': 46,\n",
       " 'npr.org': 47,\n",
       " 'baidu.com': 48,\n",
       " 'theverge.com': 49,\n",
       " 'github.com': 50,\n",
       " 'telegraph.co.uk': 51,\n",
       " 'bbc.co.uk': 52,\n",
       " 'ebay.com': 53,\n",
       " 'paypal.com': 54,\n",
       " 'pinterest.com': 55,\n",
       " 'medium.com': 56,\n",
       " 'un.org': 57}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario = 'configuration00_default'\n",
    "base_folder = f'../../data/reduced_list/{scenario}'\n",
    "create_pkl(base_folder, f\"../../data/train_test_WF/{scenario}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
